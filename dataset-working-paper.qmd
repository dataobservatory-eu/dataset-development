---
title: "Making Datasets Truly Interoperable and Reusable in R"
subtitle: "A working paper to develop the dataset R package (0.2.0)"
format: 
   html: default
   docx: default
   epub: default
   pdf: default
author:
   - name: Daniel Antal
     orcid: 0000-0003-1689-0557
editor: visual
toc: true
lang: en-GB
doi: "10.5281/zenodo.10091666"
bibliography: 
 - bib/datasetdevel.bib
 - bib/rdf.bib
 - bib/rpackages.bib
 - bib/datamodels.bib
 - bib/eXtremeDesign.bib
nocite: |
  @dublin_core_dcmi_2020, @datacite_ev_datacite_2021, @beckett_rdf_2014, @datacite_metadata_scheme_4-4, @prov-o_2013, @DCAT_2, @biron_xml_2004, @N-Quads_2014, @N-Triples_2014, @DCAT_3
---

{{< pagebreak >}}

The dataset package extension to the R statistical environment aims to ensure that the most important R object that contains a dataset, i.e. a [data.frame](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/data.frame) or an inherited [tibble](https://tibble.tidyverse.org/reference/tibble.html), [tsibble](https://tsibble.tidyverts.org/) or [data.table](https://rdatatable.gitlab.io/data.table/) contains important metadata for the reuse and validation of the dataset contents. We aim to offer a novel solution to support individuals or small groups of data scientists working in various business, academic or policy research functions who cannot count on the support of librarians, knowledge engineers, and extensive documentation processes.

The development of the _dataset_ R package started in 2022 with a very experimental first release of some code and articles about what this software package extension to the R statistical system should do. The initial package idea was released on CRAN [@antal_dataset_2023] and sent for peer review to [rOpenSci](https://github.com/ropensci/software-review/issues/553). 

In 2023, a more thorough analysis of the non-functional and functional requirements started. This document reviews the literature on possible solutions and provides guidance to start the development. 

This document accompanies the latest CRAN version 0.2.7 [@r_package_dataset], that has improved documentation with the current development version of the _dataset_ package.

_You can refer to this document with DOI [10.5281/zenodo.10091666](https://doi.org/10.5281/zenodo.10091666) On GitHub this is _release [0.1.0](https://github.com/dataobservatory-eu/dataset-development/releases/tag/0..0)_. 

## Motivation 

We want to increase the (re-)usability of datasets by making their use more efficient and linking further relevant information (new observations, new information on the observed concepts or variables) or transforming the information (for example, slicing into meaningful subsets of the dataset) easier. 

The efficiency is significantly increased if the dataset is easy to review and validate by a third party, if interested parties can find and access the dataset easily, and if the dataset offers interoperable use without modification in a wide array of computer platforms and applications. Generally, the dataset has a higher value if it is easy to reuse by the same party or new parties later. 

More generally, many datasets are extremely hard to reuse even a few months or years after their making because they are poorly documented and contain too little semantic information for reuse. Unfortunately, such datasets may be more expensive to re-validate and make usable again than being created from scratch, resulting in the wasteful repetition of the data Sisyphos. Proper, undetached storage of data and metadata about the data provenance and semantics make the datasets future-proof because many steps of the documentation and validation do not have to be re-performed before a new use. Another sense of making datasets future-proof is that they are interoperable with currently used computer platforms and in current software applications, future platforms, and future applications.

We are developing the _dataset_ package to make R user's data frames (in the form of ....) more valuable in two ways.

- [x] We value use efficiency, i.e., less processing, validation and documentation needed for use in relational databases, spreadsheets and statistical applications, and semantic (graph) databases.
- [x] We want to enhance them in a way that increases the (re-)use value of the dataset by making them future-proof, easier to find and access, and more interoperable. 

## Re-processing and re-use efficiency

Datasets are containers of systematically collected and presented data in a tabular form. Ideally, a dataset is in a "tidy" form, which is a statistical representation of Cobb's 3rd normal form. A tidy dataset is ideal to work with in statistical applications, a data processing pipeline, or an extract, transform, load (ETL) process with a relational database management system, a spreadsheet or a statistical software application. Most heavy R users will come across this concept sooner or later: the modernization of the R language and the tidyverse packages all use the concept.

A tidy dataset contains the semantics necessary for the analyst to work with the dataset in a pipeline [@wickham_tidy_2014]. While the amount of semantic information is enough for a knowledgeable data analyst who contributed to the development and exploitation of the dataset, it almost certainly needs to catch up on the need for reuse by a third party or interoperability with new applications. Interoperability and reusability can be enhanced by adding further semantic information:

- [x] Adding additional semantic information about the concepts that the variables represent allows further information to the dataset and allows the mutation of variables with external information joined from other datasets;
- [x] Adding additional semantic information about the identities of the observational helps add further observations or match new observations with existing observational units; 
- [x] Adding further semantic information about the structure of the dataset (for example, for later slicing into meaningful subsets).

The tidy data format contains the semantics necessary for the analyst to work with the dataset in a pipeline [@wickham_tidy_2014]. This amount of semantic information is usually not enough for interoperability or reuse: years after, even the initial user or analyst may struggle to understand the concepts that the data is supposed to represent; the provenance of the data becomes unclear; and a third-party user will almost certainly require further metadata to use the dataset. Like relational databases, a third-party user has difficulty accessing the data without a user-friendly schema description.

### Adding semantic information about variables 

> There are only two hard things in Computer Science: cache invalidation and naming things. 
-- Phil Karlton

In a tidy dataset, variables are in columns. An R data frame (in its form as data.frame, tibble, tsibble, data.table) allows using column names that do not contain characters reserved with a special meaning in the R language itself. Choosing good variable names is an art for the data scientist: it improves code readability, review, and later reuse. However, the use of often used (and standardised) variable names like `GEO`, `SEX`, `UNIT` is not enough for a third-party review or reuse; the same analyst will likely have problems recalling the _meaning_ of these variables without further documentation.

Let's take a look at the famous `iris` dataset. What does _Sepal.Length_ or _Species_ means? What happens if we make a mutating transformation of _Sepal.Width_? What is the unit of measure if we take the average of _Petal.Widht_?

```{r iris}
head(iris)
```

One good practice for increasing reviewability and reproducibility is to include this information in a notebook or similar document. While this is undoubtedly a good practice, often the reuse or review is hampered by the fact that the data and the documentation are separated, and for example, the dataset is available without the documentation, or worse still, the dataset is modified after the documentation was closed, but this is not visible for the reuser.

There are some R packages that help to add semantic information about the variables with a free-text variable label that allows normal punctuation and more words than the restricted col.names attribute of the data.frame. Our _dataset_ package also implements variable labelling.

```{r setup}
library(dataset)
```

```{r addlabels}
dsd_iris <- DataStructure(iris_dataset)
dsd_iris$Sepal.Length$label <- "The sepal length measured in centimeters."
dsd_iris$Sepal.Width$label  <- "The sepal width measured in centimeters."
dsd_iris$Petal.Length$label <- "The petal length measured in centimeters."
dsd_iris$Petal.Width$label  <- "The petal width measured in centimeters."
dsd_iris$Species$label      <- "The name of the Iris species in the observation."

iris_dataset_labelled <- DataStructure_update(iris_dataset, dsd_iris)
```

```{r retrievelabel}
DataStructure(iris_dataset_labelled)[["Sepal.Length"]]$label
```
Interoperability is enhanced if we know what is the valid range of the variable. Until you save the `iris_dataset_labelled` R object in native .rds form, basic interoperability is ensured: the sepal length variable will be read as a numeric variable. But even in a CSV serialisation, it is only sometimes clear to the following user's application if the variable should be read as an integer, double or a string.  The _dataset_ package allows simpler or richer range definitions to be added to each variable's structural information set.

```{r retrieverange}
DataStructure(iris_dataset_labelled)[["Sepal.Length"]]$range
```

Variable labels vary in quality, and they are natural language and context-dependent. They are not machine-actionable and require substantial time investment for interpretation or validation for reuse. 

Professional dataset producers, for example, statistical agencies, therefore encourage using controlled vocabularies, such as taxonomies or formal ontologies, to describe the meaning of the variables. Our main goal is the creation of the `dataset` S3 class that can increase the usability, reviewability or knowledge-expanding mutation capability of variables by adding standardised, machine-actionable information to the datasets.

The following example is taken from the _RDF Data Cube Vocabulary_ definition [@rdf_data_cube_2014]. Using perhaps the most human-friendly textual serialisation of the _Resource Description Framework_, i.e., Turtle [@rdf_turtle_1_1],the example below shows machine-actionable or standardised dataset-, variable-, and observation level semantic information. This serialisation should be possible to obtain from a reprocessing of the data frame itself and its metadata attributes; in other words, our `dataset()` S3 class objects must be able to carry the following semantic informatin.

```{r rdf-example-9, eval=FALSE}
# Example 9 of the RDF Data Cube Vocabulary definition

eg:dataset-le1 a qb:DataSet;
    rdfs:label "Life expectancy"@en;
    rdfs:comment "Life expectancy within Welsh Unitary authorities - extracted from Stats Wales"@en;
    qb:structure eg:dsd-le ;
    .  

eg:o1 a qb:Observation;
    qb:dataSet  eg:dataset-le1 ;
    eg:refArea                 ex-geo:newport_00pr ;                  
    eg:refPeriod               <http://reference.data.gov.uk/id/gregorian-interval/2004-01-01T00:00:00/P3Y> ;
    sdmx-dimension:sex         sdmx-code:sex-M ;
    sdmx-attribute:unitMeasure <http://dbpedia.org/resource/Year> ;
    eg:lifeExpectancy          76.7 ;
    .
```



### Adding semantic information about observations
In a tidy dataset, observations are organised into rows, and they have an identifier. The base R data.frame comes with `row.names()`. The modernised `tibble` allows the easy addition of the row identifier (as a primary key to the dataset's tabular form). The `data.table` makes indexing more efficient. The `ts` time-series object or its more modern `tsibble` form adds clear identifiers for the time dimensions of the data.

The usability of a dataset is increased if we can easily and unambiguously add new observations with `rbind(...)` or a similar function. Without logical or semantic confusion, this is only possible if the binding of new observations uses the same identifiers. Eventually, the dataset will be the most usable if the identifiers are global, persistent and unique, enabling data linking via the web.

Recalling the first observationn from the Example 9 of the _RDF Data Cube Vocabulary definition_, the `eg:` abbrevation is a shorthand of a Uniform Resource Identifier (URI) or  Internationalized Resource Identifier (IRI) in the `https://example.com/` domain. Normally, this observation identifier should resolve to a globally unique identifier which is available on the World Wide Web as a human and machine-readable identifier (with optional description.)

```{r rdf-example-9-continued, eval=FALSE}
# Example 9 of the RDF Data Cube Vocabulary definition

eg:o1 a qb:Observation;
    qb:dataSet  eg:dataset-le1 ;
    eg:refArea                 ex-geo:newport_00pr ;                  
    eg:refPeriod               <http://reference.data.gov.uk/id/gregorian-interval/2004-01-01T00:00:00/P3Y> ;
    sdmx-dimension:sex         sdmx-code:sex-M ;
    sdmx-attribute:unitMeasure <http://dbpedia.org/resource/Year> ;
    eg:lifeExpectancy          76.7 ;
    .
```

To go back to the `iris` dataset: the observation (row) identifier is certainly not globally unique because the row identifiers `1`, `2`, ... `150` are present by default in any R data frame with 150 rows. What would make them unique if the `eg:` shorthand would resolve to a unique identifier. The simplest way to create such a unique identifier is to derive the root of the observation (row) identifier from a globally unique identifier of the dataset.


```{r iris-observation-ids}
data("iris")
eg_iris <-iris
row.names(eg_iris) <- paste0("eg:o", row.names(iris))
head(eg_iris)[1:6,]
```

We placed a copy of the (semantically enhanced) version of the famous iris dataset with the digital object identifier (DOI) `10.5281/zenodo.10396807` to the Zenodo data repository that promises many decades of availability for this copy. The DOI identifier is designed so that the `https://doi.org/10.5281/zenodo.10396807` URI, as a URL, i.e., universal resource locator, dereferences to the actual location of the dataset as a resource.

```{r iris-observation-ids2}
row.names(eg_iris) <- paste0("https://doi.org/10.5281/zenodo.10396807:o", row.names(iris))
head(eg_iris)[1:6,]
```

Replacing the `eg:` shorthand or prefix to `https://doi.org/zenodo.10396807` uniquely identifies each observation (row) in our semantically enriched version of the `iris` dataset.

The _dataset_ package aims to add this functionality to R data frames to be serialised into a format to the semantic web or web of data. While the addition can be made with 0.2.8, we will develop more helper functions after user feedbacks.


### Adding semantic information about the structure of the dataset

The Statistical Data and Metadata eXchange used the Data Structure Definition to add structural information to datasets. Structural information goes beyond a semantically rich description of the variables (in columns) and observations (in rows). It contains information about the valid subsetting or extension of the dataset.

## More FAIR Datasets

So far, we have talked about extending the semantic information about the columns (variables), rows (observations) and dataset structure for greater re-usability. We now turn to findability, accessibility and increased interoperability.

A dataset on a messy server or open science repository is challenging to find. With clear use licensing information, a third party can usually legally use it. Therefore, we must add metadata about the dataset itself.

## Descriptive metadata

We designed the _dataset_ package to work with the three most widely used. Information science standards to help the findability and accessibility of datasets:

- [x] The Dublin Core standard is widely used by library repositories. It is formally standardized internationally as ISO 15836 by the International Organization for Standardization (ISO) and as IETF RFC 5013 by the Internet Engineering Task Force (IETF) [@dublin_core_dcmi_2020].

- [x] DataCite, a more dataset-centric standard that is preferred by the European open science and open data infrastructure and often used by scientific repositories (almost fully supported in 0.2.7) [@datacite_ev_datacite_2021]. 

- [x] schema.org used by internet search engines (not yet supported in 0.2.7)

### Data provenance

Furthermore, we would like to add provenance information to each dataset, because it helps review and validation. Without data provenance, the new user cannot be certain what happened with the data since primary collection, what sort of data processing, imputation or estimation took place. This usually results in not trusting the data, and re-starting the processing tasks. Another potential problem is that the user starts making unfounded assumptions about these earlier transformations which leads to erroneous use of the dataset.

The current, early version of _dataset_ uses the generic solutions of Dublin Core and DataCite to add provenance information, but with more user experience, we would like to significantly improve the documentation of the data history and sources.

### Data catalogues and APIs

The best way to make datasets findable and accessible and offer them for reuse is by placing them in standard data catalogues or APIs.

The _dataset_ package is designed to add all necessary information to an R data frame for later addition to the data catalogue that conforms to the W3C DCAT definition (for World Wide Web use) or the slightly modified European DCAT-API definition that allows the inclusion of the dataset in European open data APIs, or into the European Open Data Portal.  

In later stages, we also want to improve support for the Statistical Data and Metadata eXchange cataloguing processes (which are also aligned with the W3C DCAT.)




; 

The FAIR recommendations focus on describing the dataset as a whole when released and mandate the use of Dublin Core (DC) or DataCite descriptive metadata [@dublin_core_dcmi_2020; @datacite_ev_datacite_2021]. Our critique of the approach of describing datasets with DC or DataCite alone is that librarians developed DC with a focus on texts; DataCite, while considering datasets, also wanted to be format-agnostic and offer a solution that can describe texts, still images, video and datasets alike. While using DC and DataCite can significantly increase findability and encourage proper attribution, they could be more helpful for interoperability and reuse in the case of datasets. A user can immediately reuse a book in her domain of expertise if it is written in a natural language she understands. This is not the case with a dataset.

Data is only informative with metadata. The kind of metadata that is required to read and functionally use a dataset is not the descriptive metadata that helps find the file containing the dataset but metadata that makes the numbers and their organisation intelligible for the data analyst. How are observations organised in the dataset? What do columns mean? Are the numbers expressed in euros or dollars? Kilograms or tons? Without such semantic information, the dataset cannot be reused. It is also barely interoperable because addition into a relational or graph database is hard or impossible.

In R, most metadata packages, for example, dataspice [@dataspice] add the metadata to a new dataset, creating an initial separation of data and metadata. Our approach, i.e., adding metadata, including rich semantics, into the attributes metadata fields of an object, keeps the data and its crucial metadata attached. This creates sufficient interoperability and reuse within the R ecosystem, provided the user serialises the data and metadata into a native `.rds` file.

The problem with this approach is that for interoperability and reuse reasons, we often serialise the contents of R objects into more interoperable formats, such as CSV or Excel files. At this point, we will have to work with at least two files: a data and a metadata file. For example, the W3C recommendation for releasing data in CSV format is to accompany the data with semantic and other metadata in an accompanying JSON file [@tennison_csv_2016].

A broader problem that we will describe in more detail is that a dataset, even a _tidy dataset_ does not contain enough semantic information for re-use. A reuse by a third party, or even by the creator of the dataset after 1-2 years requires at least a precise description of the _meaning_ of variables and _codelists_. Various metadata standards and models, such as _DCAT_ or the _W3C Data cube_ (strongly aligned with the SDMX Datacube model) and _Schema.org_ require additional metadata on the codelists and variable concepts used in the dataset. [@DCAT_2; @cyganiak_rdf_2014] Additionally, they recognise the fact that for greater interoperability, the same dataset may be serialised into different file formats (for example, `.csv`, `.xlsx`, and `.sav` files) with essentially the same meaning but conforming the differnet ways how these file formats represent data and metadata. While DataCite and Dublin Core may create a separate data record for each file verison, it is important to connect the same data file with different file formats in a data catalogue. Even in Dublin Core or DataCite, a diligent record keeping would record the _relation_ of the CSV and Excel representation of the dataset with the very same meaning.

Therefore, we would like to provide an intermediate R user with the opportunity to correctly record metadata for greater interoperability and real third-party reuse. In a well-regulated working environment with large IT and documentation resources, our problems are well handled by extensive processes and procedures. GSIM is a statistical vocabulary that harmonises the uses of the SDMX statistical data standard [@sdmx_information_model_2021]
and the DDI [@ddi_alliance_ddi-codebook_2012], a standard for documenting, coding and maintaining microdata files.  Most R users do not work in such a well-regulated environment, and we want to give them a practical tool to improve their process documentation for much-improved interoperability and reusability with the introduction of the dataset R package [@antal_dataset_2023].

## Data Linking & Serialisation

The _dataset_ package is currently written entirely in the R language. It is planned to offer serialisations to metadata languages, starting with the `Turtle 1.1` _Terse RDF Triple Language_ (retaining compatibility with the 1.2 version) and with the `N-Quads 1.1` standard [@rdf_turtle_1_1; @rdf_turtle_1_2;  @rdf_n-quads_1_1]. This is currently achieved in the experimental `convert_to_nquad()` function, which is not yet packaged to _dataset_. 

## Methodology
Because of the unavailability of our initial research and development partner, we started to work with the University of Bologna and King's College London and adopted their design practices. As the project aimed to incorporate formal computer ontologies,  we sought a methodology that fits well with eXtreme Design, an ontology engineering methodology used by our partners. They used a UX-extended version of eXtreme Design (XD) [@blomqvist_experimenting_2010; @blomqvist_engineering_2016]. The eXtreme Design methdology itself is rooted in Ontology Design Patterns [@gangemi_ontology_200], which is an adoptation of software design patterns to ontologies.

XD provides support to incrementally address small sets of requirements formalised as competency questions (CQs). While XD is an ontology design methodology, it is essentially an iterative design methodology that uses software design patterns. We needed very little modification in eXtreme Design: it was applied to provide ontology design patterns described in the OWL/RDF metadata languages that we further needed to extend with R language functional software. After all, the aim of _dataset_ is to write software that follows strict dataset templates to represent data stored in relational or graph databases or stand-alone spreadsheets or statistical software files in a semantically richer format. 

The use of XD was particularly useful to us, because it had been applied in the he cultural heritage [@carriero_pattern-based_2021], and our beachhead market was the application of music heritage and rights management data.




## References
